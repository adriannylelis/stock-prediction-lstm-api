# RelatÃ³rio de Testes End-to-End

## ðŸ“‹ VisÃ£o Geral

Este documento descreve a estratÃ©gia de testes end-to-end implementada para validar os fluxos completos de **treino**, **retreino** e **prediÃ§Ã£o** do sistema de ML.

---

## ðŸŽ¯ Objetivos dos Testes

1. âœ… Validar fluxos completos de ponta a ponta
2. âœ… Garantir integraÃ§Ã£o correta entre componentes
3. âœ… Verificar persistÃªncia e versionamento de dados
4. âœ… Testar detecÃ§Ã£o de drift e gatilho de retreino
5. âœ… Assegurar reprodutibilidade dos resultados

---

## ðŸ“Š Cobertura de Testes

### **EstatÃ­sticas Finais**
- âœ… **83 testes** implementados
- âœ… **100% passando** (83/83)
- ðŸ“ˆ **72.79% de cobertura** de cÃ³digo
- â±ï¸ **Tempo de execuÃ§Ã£o**: ~1min 30s

### **DistribuiÃ§Ã£o dos Testes**

| Categoria | Quantidade | Status |
|-----------|------------|--------|
| **Integration Tests** | 8 | âœ… 100% |
| **Unit Tests** | 75 | âœ… 100% |
| **Pipeline Tests** | 4 | âœ… 100% |
| **Monitoring Tests** | 31 | âœ… 100% |

---

## ðŸ”„ Testes End-to-End de Treino

### **1. Test: `test_train_pipeline_end_to_end`**

**Objetivo**: Validar o pipeline completo de treinamento desde a ingestÃ£o de dados atÃ© o salvamento do modelo.

**Fluxo Testado**:
```
Data Ingestion â†’ Feature Engineering â†’ Preprocessing â†’ Training â†’ Evaluation â†’ Save
```

**ImplementaÃ§Ã£o**:
```python
def test_train_pipeline_end_to_end(temp_artifacts_dir):
    """Test complete training pipeline from data ingestion to model save."""
    
    # 1. Setup: Create pipeline with configuration
    pipeline = TrainPipeline(
        ticker="PETR4.SA",
        start_date="2023-01-01",
        end_date="2023-12-31",
        lookback=30,
        hidden_size=32,
        num_layers=1,
        epochs=2,  # Reduced for testing speed
        batch_size=32,
        model_save_path=f"{temp_artifacts_dir}/best_model.pt",
        experiment_name="test_experiment"
    )
    
    # 2. Execute: Run complete pipeline
    results = pipeline.run()
    
    # 3. Validate: Check all expected outputs
    assert 'model_path' in results
    assert 'training_history' in results
    assert 'test_metrics' in results
    assert 'metadata' in results
    
    # 4. Verify: Model file exists
    assert Path(results['model_path']).exists()
    
    # 5. Check: Metrics keys (uppercase: MAE, RMSE, MAPE, R2)
    assert 'RMSE' in results['test_metrics']
    assert 'MAE' in results['test_metrics']
    assert 'MAPE' in results['test_metrics']
    assert 'R2' in results['test_metrics']
    
    # 6. Validate: Training history structure
    assert len(results['training_history']['train_loss']) == 2
    assert len(results['training_history']['val_loss']) == 2
```

**ValidaÃ§Ãµes**:
- âœ… Download de 248 registros do Yahoo Finance
- âœ… CriaÃ§Ã£o de 19 features (14 indicadores tÃ©cnicos)
- âœ… NormalizaÃ§Ã£o e criaÃ§Ã£o de sequÃªncias (218 sequÃªncias de 30 dias)
- âœ… Split: 152 train / 32 val / 34 test
- âœ… Treinamento por 2 Ã©pocas com early stopping
- âœ… Salvamento do modelo como `best_model.pt`
- âœ… Salvamento do scaler para prediÃ§Ãµes
- âœ… MÃ©tricas calculadas: MAE, RMSE, MAPE, RÂ², Directional Accuracy
- âœ… Metadata completa com configuraÃ§Ãµes e timestamps

**Tempo de ExecuÃ§Ã£o**: ~15 segundos

---

## ðŸ”„ Testes End-to-End de Retreino

### **2. Test: `test_full_retraining_workflow`**

**Objetivo**: Simular workflow completo de produÃ§Ã£o com detecÃ§Ã£o de drift e retreino automÃ¡tico.

**Fluxo Testado**:
```
Train V1 â†’ Monitor Data â†’ Detect Drift â†’ Trigger Retrain â†’ Train V2 â†’ Compare Models
```

**ImplementaÃ§Ã£o**:
```python
def test_full_retraining_workflow(clean_artifacts):
    """Test complete retraining workflow with drift detection."""
    
    # === STEP 1: Train Initial Model ===
    pipeline_v1 = TrainPipeline(
        ticker="PETR4.SA",
        start_date="2023-01-01",
        end_date="2023-03-01",
        lookback=10,
        epochs=2,
        model_save_path="artifacts_test/v1/best_model.pt",
        experiment_name=None
    )
    
    results_v1 = pipeline_v1.run()
    assert Path(results_v1["model_path"]).exists()
    
    # === STEP 2: Simulate Production Data ===
    # Reference data (training distribution)
    np.random.seed(123)
    ref_data = pd.DataFrame({
        "Close": np.random.normal(30, 5, 200),
        "Volume": np.random.normal(1e6, 2e5, 200)
    })
    
    # New production data (with drift)
    new_data = pd.DataFrame({
        "Close": np.random.normal(33, 5, 200),  # Mean shifted +3
        "Volume": np.random.normal(1.1e6, 2e5, 200)  # Mean shifted +10%
    })
    
    # === STEP 3: Detect Drift ===
    detector = DriftDetector()
    drift_report = detector.detect_drift(ref_data, new_data)
    
    assert drift_report["has_drift"] is True
    assert "Close" in drift_report["drifted_features"]
    
    # === STEP 4: Trigger Retraining ===
    if drift_report["has_drift"]:
        pipeline_v2 = TrainPipeline(
            ticker="PETR4.SA",
            start_date="2023-02-01",  # Updated time window
            end_date="2023-04-01",
            lookback=10,
            epochs=2,
            model_save_path="artifacts_test/v2/best_model.pt",
            experiment_name=None
        )
        
        results_v2 = pipeline_v2.run()
        
        # === STEP 5: Validate New Model ===
        assert Path(results_v2["model_path"]).exists()
        
        # Verify models are in different directories
        assert results_v2["model_path"] != results_v1["model_path"]
        
        # Both models exist simultaneously
        assert Path(results_v1["model_path"]).exists()
        assert Path(results_v2["model_path"]).exists()
```

**ValidaÃ§Ãµes**:
- âœ… Modelo V1 treinado e salvo com sucesso
- âœ… Drift detectado em feature `Close` (KS-test)
- âœ… Modelo V2 retreinado com dados atualizados
- âœ… Ambos os modelos coexistem (versionamento)
- âœ… Paths diferentes para V1 e V2
- âœ… Metadata registra versÃµes e timestamps

**CenÃ¡rios de Drift Testados**:

| Tipo de Drift | MÃ©todo | Threshold | Detectado |
|---------------|--------|-----------|-----------|
| **Distribution Shift** | Kolmogorov-Smirnov | 0.05 | âœ… |
| **Population Stability** | PSI | 0.1 | âœ… |
| **Feature Drift** | Statistical Tests | Custom | âœ… |

**Tempo de ExecuÃ§Ã£o**: ~25 segundos

---

## ðŸ”® Testes End-to-End de PrediÃ§Ã£o

### **3. Test: `test_predict_pipeline_end_to_end`**

**Objetivo**: Validar o pipeline completo de prediÃ§Ã£o desde o carregamento do modelo atÃ© a geraÃ§Ã£o de previsÃµes.

**Fluxo Testado**:
```
Load Model â†’ Ingest Latest Data â†’ Feature Engineering â†’ Preprocess â†’ Predict â†’ Return Results
```

**ImplementaÃ§Ã£o**:
```python
def test_predict_pipeline_end_to_end(temp_artifacts_dir):
    """Test complete prediction pipeline."""
    
    # === STEP 1: Train a Model First ===
    train_pipeline = TrainPipeline(
        ticker="PETR4.SA",
        start_date="2023-01-01",
        end_date="2023-12-31",
        lookback=30,
        hidden_size=32,
        epochs=2,
        model_save_path=f"{temp_artifacts_dir}/pred_test_model.pt"
    )
    train_results = train_pipeline.run()
    
    # === STEP 2: Create Prediction Pipeline ===
    predict_pipeline = PredictPipeline(
        model_path=train_results['model_path'],
        ticker="PETR4.SA",
        lookback=30
    )
    
    # === STEP 3: Generate Predictions ===
    predictions_df = predict_pipeline.predict(days_ahead=5)
    
    # === STEP 4: Validate Predictions ===
    assert isinstance(predictions_df, pd.DataFrame)
    assert 'Date' in predictions_df.columns
    assert 'Predicted_Close' in predictions_df.columns
    assert len(predictions_df) == 5  # 5 days ahead
    assert predictions_df['Predicted_Close'].notna().all()
    
    # === STEP 5: Validate Data Types ===
    assert predictions_df['Predicted_Close'].dtype in [np.float64, np.float32]
    
    # === STEP 6: Validate Date Range ===
    # Predictions should be for future dates
    assert predictions_df['Date'].is_monotonic_increasing
```

**ValidaÃ§Ãµes**:
- âœ… Modelo carregado com arquitetura completa
- âœ… Download de dados dos Ãºltimos 2 anos (499 registros)
- âœ… Features tÃ©cnicos calculados automaticamente
- âœ… NormalizaÃ§Ã£o usando scaler salvo do treino
- âœ… SequÃªncia preparada corretamente (1, 30, 1)
- âœ… 5 prediÃ§Ãµes geradas com sucesso
- âœ… Formato de saÃ­da correto (DataFrame com Date e Predicted_Close)
- âœ… Valores numÃ©ricos vÃ¡lidos (sem NaN)
- âœ… Datas em ordem crescente

**Exemplo de Output**:
```
        Date  Predicted_Close
0 2025-12-27        29.958292
1 2025-12-28        29.949803
2 2025-12-29        29.942673
3 2025-12-30        29.939136
4 2025-12-31        29.937384
```

**Tempo de ExecuÃ§Ã£o**: ~8 segundos

---

## ðŸ”„ Teste de IntegraÃ§Ã£o: Train â†’ Predict

### **4. Test: `test_train_and_predict_integration`**

**Objetivo**: Validar integraÃ§Ã£o completa entre pipelines de treino e prediÃ§Ã£o.

**Fluxo Testado**:
```
Train Model â†’ Save Scaler â†’ Load Model â†’ Make Predictions â†’ Validate Results
```

**ImplementaÃ§Ã£o**:
```python
def test_train_and_predict_integration(temp_artifacts_dir):
    """Test training and prediction work together."""
    
    # === TRAIN PHASE ===
    train_pipeline = TrainPipeline(
        ticker="VALE3.SA",
        start_date="2023-01-01",
        end_date="2023-06-30",
        lookback=20,
        hidden_size=16,
        epochs=1,
        model_save_path=f"{temp_artifacts_dir}/integration_model.pt"
    )
    train_results = train_pipeline.run()
    
    # Verify scaler was saved alongside model
    scaler_path = Path(train_results['model_path']).parent / "scaler.pkl"
    assert scaler_path.exists()
    
    # === PREDICT PHASE ===
    predict_pipeline = PredictPipeline(
        model_path=train_results['model_path'],
        ticker="VALE3.SA",
        lookback=20
    )
    predictions = predict_pipeline.predict(days_ahead=3)
    
    # === VALIDATION ===
    assert len(predictions) > 0
    assert predictions['Predicted_Close'].dtype in [np.float64, np.float32]
```

**ValidaÃ§Ãµes**:
- âœ… Scaler salvo no mesmo diretÃ³rio do modelo
- âœ… Modelo carregado sem erros
- âœ… PrediÃ§Ãµes geradas usando mesmo scaler do treino
- âœ… ConsistÃªncia entre normalizaÃ§Ã£o de treino e prediÃ§Ã£o

---

## ðŸ“Š Testes de Monitoramento

### **5. Test: `test_drift_detection_workflow`**

**Objetivo**: Validar sistema de detecÃ§Ã£o de drift em produÃ§Ã£o.

**ImplementaÃ§Ã£o**:
```python
def test_drift_detection_workflow():
    """Test drift detection workflow."""
    
    detector = DriftDetector(ks_threshold=0.05, psi_threshold=0.1)
    
    # Reference data (training distribution)
    np.random.seed(42)
    ref_data = pd.DataFrame({
        'feature1': np.random.normal(0, 1, 1000),
        'feature2': np.random.normal(5, 2, 1000)
    })
    
    # Production data with drift
    prod_data = pd.DataFrame({
        'feature1': np.random.normal(1, 1, 1000),  # Mean shifted
        'feature2': np.random.normal(5, 2, 1000)   # No drift
    })
    
    # Detect drift using KS-test
    ks_report = detector.detect_drift(ref_data, prod_data)
    assert ks_report['has_drift'] is True
    assert 'feature1' in ks_report['drifted_features']
    
    # Detect drift using PSI
    psi_report = detector.detect_drift_psi(ref_data, prod_data)
    assert isinstance(psi_report['feature_psi'], dict)
    assert 'feature1' in psi_report['feature_psi']
```

**ValidaÃ§Ãµes**:
- âœ… KS-test detecta drift em distribuiÃ§Ã£o
- âœ… PSI calcula estabilidade populacional
- âœ… Features especÃ­ficos identificados
- âœ… Threshold configurÃ¡vel
- âœ… Report estruturado com scores

---

## ðŸ“Š Testes de Versionamento

### **6. Test: `test_data_versioning_workflow`**

**Objetivo**: Validar sistema de versionamento de dados.

**ImplementaÃ§Ã£o**:
```python
def test_data_versioning_workflow(temp_data_dir):
    """Test data versioning and loading workflow."""
    
    manager = DataVersionManager(
        base_path=temp_data_dir,
        auto_cleanup=True,
        max_versions=3
    )
    
    # Create test data
    test_data = pd.DataFrame({
        'Close': np.random.random(100),
        'Volume': np.random.randint(1000, 10000, 100)
    })
    
    # Save multiple versions
    versions = []
    for i in range(5):
        version = manager.save(
            test_data,
            ticker="TEST.SA",
            metadata={'iteration': i}
        )
        versions.append(version)
    
    # Check auto-cleanup (should keep only 3)
    remaining = manager.list_versions("TEST.SA")
    assert len(remaining) <= 3
    
    # Load latest
    loaded_df = manager.load_latest("TEST.SA")
    assert len(loaded_df) == 100
```

**ValidaÃ§Ãµes**:
- âœ… Versionamento com timestamp + milissegundos
- âœ… Auto-cleanup funciona (mantÃ©m max_versions)
- âœ… Load de versÃ£o especÃ­fica
- âœ… Load da versÃ£o mais recente
- âœ… Metadata preservada

---

## ðŸ› ï¸ EstratÃ©gias de Teste Implementadas

### **1. Fixtures ReutilizÃ¡veis**

```python
@pytest.fixture
def temp_artifacts_dir(tmp_path):
    """Create temporary artifacts directory."""
    artifacts_dir = tmp_path / "artifacts"
    artifacts_dir.mkdir()
    yield str(artifacts_dir)
    if artifacts_dir.exists():
        shutil.rmtree(artifacts_dir)

@pytest.fixture
def clean_artifacts():
    """Clean up artifacts before and after tests."""
    # Cleanup before
    # ... yield ...
    # Cleanup after
```

### **2. Markers para CategorizaÃ§Ã£o**

```python
@pytest.mark.integration  # Integration tests
@pytest.mark.unit         # Unit tests
@pytest.mark.slow         # Slow tests (skip in CI)
```

### **3. ParametrizaÃ§Ã£o**

```python
@pytest.mark.parametrize("ticker,lookback", [
    ("PETR4.SA", 10),
    ("VALE3.SA", 20),
    ("BBAS3.SA", 30),
])
def test_train_multiple_tickers(ticker, lookback):
    # Test with different parameters
```

### **4. Mocking para Isolamento**

```python
@patch('src.ml.data.ingestion.yf.download')
def test_ingestion_with_mock(mock_download):
    mock_download.return_value = mock_dataframe
    # Test without external API calls
```

---

## ðŸ“ˆ Cobertura Detalhada

### **MÃ³dulos com Alta Cobertura (>80%)**

| MÃ³dulo | Cobertura | Linhas Testadas |
|--------|-----------|-----------------|
| `train_pipeline.py` | **100%** | 133/133 |
| `predict_pipeline.py` | **93.10%** | 81/87 |
| `feature_engineering.py` | **89.69%** | 87/97 |
| `lstm.py` | **83.87%** | 26/31 |

### **MÃ³dulos com Cobertura MÃ©dia (50-80%)**

| MÃ³dulo | Cobertura | Prioridade |
|--------|-----------|------------|
| `preprocessing.py` | 72.29% | MÃ©dia |
| `trainer.py` | 71.32% | Alta |
| `metrics.py` | 69.39% | MÃ©dia |

### **MÃ³dulos com Baixa Cobertura (<50%)**

| MÃ³dulo | Cobertura | Motivo |
|--------|-----------|--------|
| `persistence.py` | 47.24% | Muitos mÃ©todos de I/O |
| `experiment_tracker.py` | 45.45% | IntegraÃ§Ã£o com MLflow |
| `hyperparameter_tuner.py` | 20.25% | Optuna (testes lentos) |
| `device.py` | 24.24% | GPU nÃ£o disponÃ­vel em CI |

---

## ðŸ› Problemas Encontrados e SoluÃ§Ãµes

### **Problema 1: Model Path Inconsistente**
**Sintoma**: `FileNotFoundError: Model not found`  
**Causa**: Trainer salvava como `best_model.pt`, mas TrainPipeline retornava path original  
**SoluÃ§Ã£o**: Modificado `_save_results()` para retornar path real do arquivo salvo

```python
# ANTES
'model_path': str(self.model_save_path)

# DEPOIS
actual_model_path = Path(self.model_save_path).parent / 'best_model.pt'
'model_path': str(actual_model_path)
```

### **Problema 2: Checkpoint Incompleto**
**Sintoma**: `KeyError: 'input_size'`  
**Causa**: Trainer nÃ£o salvava arquitetura do modelo  
**SoluÃ§Ã£o**: Adicionado arquitetura completa ao checkpoint

```python
checkpoint = {
    'epoch': epoch + 1,
    'model_state_dict': self.model.state_dict(),
    'optimizer_state_dict': self.optimizer.state_dict(),
    'best_val_loss': self.best_val_loss,
    'history': self.history,
    # Adicionado:
    'input_size': self.model.input_size,
    'hidden_size': self.model.hidden_size,
    'num_layers': self.model.num_layers,
    'dropout': self.model.dropout_prob
}
```

### **Problema 3: PyTorch 2.6 Weights Only**
**Sintoma**: `UnpicklingError: Weights only load failed`  
**Causa**: PyTorch 2.6 mudou default de `weights_only` para `True`  
**SoluÃ§Ã£o**: Explicitamente definir `weights_only=False`

```python
checkpoint = torch.load(self.model_path, map_location=self.device, weights_only=False)
```

### **Problema 4: PredictPipeline API Mismatch**
**Sintoma**: `TypeError: unexpected keyword argument 'start_date'`  
**Causa**: Testes usando API antiga do PredictPipeline  
**SoluÃ§Ã£o**: Atualizar testes para usar API correta (model_path, ticker, lookback)

---

## âœ… ConclusÃµes

### **Pontos Fortes**
1. âœ… **Cobertura Abrangente**: 83 testes cobrindo todos os fluxos principais
2. âœ… **Reprodutibilidade**: Seeds fixos garantem resultados consistentes
3. âœ… **Isolamento**: Fixtures garantem independÃªncia entre testes
4. âœ… **Velocidade**: ~1min 30s para suite completa
5. âœ… **Debugging**: Scripts de debug facilitam troubleshooting

### **Melhorias Implementadas**
1. âœ… Ruff substituiu 4 ferramentas (black, isort, flake8, mypy)
2. âœ… Testes de integraÃ§Ã£o completos para todos os pipelines
3. âœ… Versionamento de dados testado
4. âœ… Drift detection validado
5. âœ… PersistÃªncia de artefatos verificada

### **PrÃ³ximos Passos**
- [ ] Aumentar cobertura para 80%+ (atualmente 72.79%)
- [ ] Adicionar testes de performance/benchmarking
- [ ] Implementar testes de carga
- [ ] Adicionar property-based testing (Hypothesis)
- [ ] Configurar CI/CD com GitHub Actions

---

**VersÃ£o**: 1.0.0  
**Ãšltima AtualizaÃ§Ã£o**: 28/12/2025  
**Total de Testes**: 83 (100% passando)  
**Cobertura**: 72.79%
