# @package _global_

# Training configuration

training:
  # Optimization
  optimizer: Adam
  learning_rate: 0.001
  weight_decay: 0.0  # L2 regularization
  
  # Loss function
  loss_function: MSE  # Options: MSE, MAE, Huber
  
  # Training epochs
  epochs: 100
  batch_size: 32
  
  # Early stopping
  early_stopping:
    enable: true
    patience: 10  # Number of epochs to wait before stopping
    min_delta: 0.0001  # Minimum improvement to consider
    monitor: val_loss  # Metric to monitor
  
  # Learning rate scheduler
  lr_scheduler:
    enable: false
    type: ReduceLROnPlateau  # Options: ReduceLROnPlateau, StepLR, CosineAnnealingLR
    factor: 0.5  # Multiplicative factor for LR reduction
    patience: 5  # Epochs with no improvement before reducing LR
    min_lr: 0.00001
  
  # Gradient clipping (prevent exploding gradients)
  gradient_clipping:
    enable: false
    max_norm: 1.0
  
  # Checkpointing
  checkpoint:
    save_best_only: true
    save_frequency: 10  # Save every N epochs
    monitor: val_loss
    mode: min  # Options: min, max
  
  # Validation
  validate_every: 1  # Validate every N epochs
  
  # Logging frequency
  log_every_n_steps: 10  # Log metrics every N batches
